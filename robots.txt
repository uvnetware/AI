User-agent: *
# Allow all search engines to crawl the site

# Block access to sensitive or irrelevant directories
Disallow: /admin/
Disallow: /login/
Disallow: /register/
Disallow: /cgi-bin/
Disallow: /private/
Disallow: /tmp/
Disallow: /test/
Disallow: /backup/
Disallow: /dev/
Disallow: /staging/
Disallow: /wp-admin/
Disallow: /wp-includes/

# Block access to specific file types that do not contribute to SEO
Disallow: /*.pdf$
Disallow: /*.zip$
Disallow: /*.doc$
Disallow: /*.exe$
Disallow: /*.bak$
Disallow: /*.old$
Disallow: /*.log$

# Block access to URL parameters that create duplicate content
Disallow: /*?replytocom
Disallow: /*?sessionid=
Disallow: /*?ref=
Disallow: /*?utm_source=
Disallow: /*?utm_medium=
Disallow: /*?utm_campaign=

# Block crawling of low-value content
Disallow: /search/
Disallow: /tag/
Disallow: /category/
Disallow: /archive/
Disallow: /page/
Disallow: /print/

# Allow indexing of dynamic content generated by AJAX or JavaScript
Allow: /*.js$
Allow: /*.css$
Allow: /*.jpg$
Allow: /*.png$
Allow: /*.webp$
Allow: /*.svg$
Allow: /*.mp4$

# Priority crawling for high-value content
Allow: /blog/
Allow: /news/
Allow: /products/
Allow: /services/

# Sitemap locations for comprehensive indexing
Sitemap: https://ai.uvnetware.com/sitemap.xml
Sitemap: https://ai.uvnetware.com/blog-sitemap.xml
Sitemap: https://ai.uvnetware.com/news-sitemap.xml
Sitemap: https://ai.uvnetware.com/products-sitemap.xml

# Specify crawl delay to optimize server load and crawling efficiency
Crawl-delay: 5

# Optimize for Googlebot (Google's main crawler)
User-agent: Googlebot
Disallow: /nogoogle/
Allow: /google-specific-content/
Allow: /*.js$
Allow: /*.css$
Allow: /*.svg$
Allow: /*.webp$
Allow: /*.jpg$
Allow: /*.png$

# Specific rules for Bingbot (Microsoft's search engine crawler)
User-agent: Bingbot
Disallow: /nobing/
Allow: /bing-specific-content/
Crawl-delay: 10

# Block bad bots and resource-heavy crawlers
User-agent: MJ12bot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: SEMrushBot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: BLEXBot
Disallow: /

User-agent: Baiduspider
Disallow: /

# Block specific user agents known for scraping or high resource usage
User-agent: YandexBot
Disallow: /

User-agent: Sogou
Disallow: /

User-agent: 80legs
Disallow: /

User-agent: DataForSeoBot
Disallow: /

# Allow for all other search engines while prioritizing high-value content
User-agent: *
Allow: /content/
Allow: /assets/
Allow: /*.js$
Allow: /*.css$
Allow: /*.jpg$
Allow: /*.png$
Allow: /*.svg$
Allow: /*.webp$

# Additional performance optimization by blocking crawlers during peak traffic hours
User-agent: *
Crawl-delay: 10

# Prevent indexing of site search results, specific tags, or categories
Disallow: /?s=
Disallow: /tag/
Disallow: /category/

# Allow Googlebot to crawl the AJAX content
User-agent: Googlebot
Allow: /ajax/
Allow: /*.js
Allow: /*.css

# Enable image indexing for all crawlers
User-agent: *
Allow: /images/

# Block entire site in case of emergency (use with caution)
# Disallow: /

# End of robots.txt




//REMOVE UNWANTED TEXT User-agent: *
# Allow all search engines to crawl the site

# Block access to sensitive or irrelevant directories
Disallow: /admin/
Disallow: /login/
Disallow: /register/
Disallow: /cgi-bin/
Disallow: /private/
Disallow: /tmp/
Disallow: /test/
Disallow: /backup/
Disallow: /dev/
Disallow: /staging/
Disallow: /wp-admin/
Disallow: /wp-includes/

# Block access to specific file types that do not contribute to SEO
Disallow: /*.pdf$
Disallow: /*.zip$
Disallow: /*.doc$
Disallow: /*.exe$
Disallow: /*.bak$
Disallow: /*.old$
Disallow: /*.log$

# Block access to URL parameters that create duplicate content
Disallow: /*?replytocom
Disallow: /*?sessionid=
Disallow: /*?ref=
Disallow: /*?utm_source=
Disallow: /*?utm_medium=
Disallow: /*?utm_campaign=

# Block crawling of low-value content
Disallow: /search/
Disallow: /tag/
Disallow: /category/
Disallow: /archive/
Disallow: /page/
Disallow: /print/

# Allow indexing of dynamic content generated by AJAX or JavaScript
Allow: /*.js$
Allow: /*.css$
Allow: /*.jpg$
Allow: /*.png$
Allow: /*.webp$
Allow: /*.svg$
Allow: /*.mp4$

# Priority crawling for high-value content
Allow: /blog/
Allow: /news/
Allow: /products/
Allow: /services/

# Sitemap locations for comprehensive indexing
Sitemap: https://ai.uvnetware.com/sitemap.xml

# Specify crawl delay to optimize server load and crawling efficiency
Crawl-delay: 5

# Optimize for Googlebot (Google's main crawler)
User-agent: Googlebot
Disallow: /nogoogle/
Allow: /google-specific-content/
Allow: /*.js$
Allow: /*.css$
Allow: /*.svg$
Allow: /*.webp$
Allow: /*.jpg$
Allow: /*.png$

# Specific rules for Bingbot (Microsoft's search engine crawler)
User-agent: Bingbot
Disallow: /nobing/
Allow: /bing-specific-content/
Crawl-delay: 10

# Block bad bots and resource-heavy crawlers
User-agent: MJ12bot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: SEMrushBot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: BLEXBot
Disallow: /

User-agent: Baiduspider
Disallow: /

# Block specific user agents known for scraping or high resource usage
User-agent: YandexBot
Disallow: /

User-agent: Sogou
Disallow: /

User-agent: 80legs
Disallow: /

User-agent: DataForSeoBot
Disallow: /

# Allow for all other search engines while prioritizing high-value content
User-agent: *
Allow: /content/
Allow: /assets/
Allow: /*.js$
Allow: /*.css$
Allow: /*.jpg$
Allow: /*.png$
Allow: /*.svg$
Allow: /*.webp$

# Additional performance optimization by blocking crawlers during peak traffic hours
User-agent: *
Crawl-delay: 10

# Prevent indexing of site search results, specific tags, or categories
Disallow: /?s=
Disallow: /tag/
Disallow: /category/

# Allow Googlebot to crawl the AJAX content
User-agent: Googlebot
Allow: /ajax/
Allow: /*.js
Allow: /*.css

# Enable image indexing for all crawlers
User-agent: *
Allow: /images/

# Block entire site in case of emergency (use with caution)
# Disallow: /

# End of robots.txt
